{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Unsupervised NLP - Encoder Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages (fastai for optimizing inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import pandas as pd\n",
    "from fastai.callbacks.tracker import *\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "from fastai.distributed import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"proc\"></a>\n",
    "#### Cleaning methods, just add terms to remove in `to_remove`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [\"\\d+\", \"\\r\", \"\\n\",\"IMPRESSION:\"]\n",
    "def clean_impressions(data,column):\n",
    "    data[column] = data[column].str.strip()\n",
    "    for item in to_remove:\n",
    "        data[column] = data[column].str.replace(item, '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_path = '../data/reports_all.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_data = pd.read_csv(text_file_path,index_col=0);all_text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_all_text_data = clean_impressions(all_text_data,'report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"shuffle_split_dataset\"></a>\n",
    "#### `shuffle_split_dataset` takes in five inputs:\n",
    "- **data** = pandas dataframe\n",
    "- **train/val/test_frac** = ratio for each development set\n",
    "- **seed** = random seed for mantain consistency in testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_split_dataset(data,train_frac=.85,val_frac=.1,test_frac=.05,seed=42):\n",
    "    \n",
    "    data = data.sample(frac=1,random_state=seed)\n",
    "    train_index = int(data.count()[0]*train_frac)\n",
    "    val_index = train_index + int(data.count()[0]*val_frac)\n",
    "    \n",
    "    train_set = data[:train_index]\n",
    "    val_set = data[train_index:val_index]\n",
    "    test_set = data[val_index:]\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = shuffle_split_dataset(cleaned_all_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to add methods to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "def add_method(cls):\n",
    "    def decorator(func):\n",
    "        @wraps(func) \n",
    "        def wrapper(self, *args, **kwargs): \n",
    "            return func(*args, **kwargs)\n",
    "        setattr(cls, func.__name__, wrapper)\n",
    "        return func \n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"token\"></a>\n",
    "#### Tokenization - `_get_processor`\n",
    "\n",
    "- **xxunk** = unknown word \n",
    "- **xxbos** = beginning of a report \n",
    "- **xxfld** = separate columns of text \n",
    "- **xxmaj** = uppercase letter \n",
    "- **xxup** = all caps \n",
    "- **xxrep n {char}** = next character is repeated n times \n",
    "- **xxwrep n {word}** = next word is repeated n times\n",
    "\n",
    "The tokenization strategy is set to [scapy](https://spacy.io/api/), but a custom version can be inputted into method. This method is input into the [`report_slicer` method](#report_slicer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_processor(tokenizer:Tokenizer=None, vocab:Vocab=None, chunksize:int=10000, max_vocab:int=60000,\n",
    "                   min_freq:int=2, mark_fields:bool=False, include_bos:bool=True, include_eos:bool=False):\n",
    "    return [TokenizeProcessor(tokenizer=tokenizer, chunksize=chunksize, \n",
    "                              mark_fields=mark_fields, include_bos=include_bos, include_eos=include_eos),\n",
    "            NumericalizeProcessor(vocab=vocab, max_vocab=max_vocab, min_freq=min_freq)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"report_slicer\"></a>\n",
    "#### `report_slicer` processes the raw text data into model-readable form\n",
    "[`TextLMDataBunch`](https://docs.fast.ai/text.data.html) is a fastai class that manages data for development, and makes it a lot easier to train. Report Slicer uses the class to store the data so that the language model can actually take in individual vocab tokens as inputs. Report Slicer's inputs are:\n",
    "\n",
    "- **cls** = class (always TextLMDataBunch)\n",
    "- **path** = directory where data is stored and where saved model files will be saved and loaded\n",
    "- **train/valid/test_df** = development sets from [`shuffle_split_dataset` method](#shuffle_split_dataset) \n",
    "- **text_cols** = the header of the column with text (can be a list of multiple columns of text if necessary, in which case, `mark_fields` should be set to <font color=blue>True</font>\n",
    "- **tokenizer** = tokenizer function (set in [`_get_processor`](#token))\n",
    "- **vocab** = not required, list of vocab to corresponding, method will automatically create initially\n",
    "- **chunksize** = iterator that loads portions of dataframe for processing, given # of characters per 'chunk'\n",
    "- **max_vocab** = max vocab in dataset\n",
    "- **mark_fields** = token to mark seperation between more than one text column for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(TextLMDataBunch)\n",
    "def report_slicer(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "                label_cols:IntsOrStrs=0, label_delim:str=None, chunksize:int=10000, max_vocab:int=60000,\n",
    "                min_freq:int=2, mark_fields:bool=False, include_bos:bool=True, include_eos:bool=False, **kwargs) -> DataBunch:\n",
    "    \n",
    "        processor = _get_processor(tokenizer=tokenizer, vocab=vocab, chunksize=chunksize, max_vocab=max_vocab,\n",
    "                                   min_freq=min_freq, mark_fields=mark_fields, \n",
    "                                   include_bos=include_bos, include_eos=include_eos)\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        if cls==TextLMDataBunch: src = src.label_for_lm()\n",
    "        else: \n",
    "            if label_delim is not None: src = src.label_from_df(cols=label_cols, classes=classes, label_delim=label_delim)\n",
    "            else: src = src.label_from_df(cols=label_cols, classes=classes)\n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        return src.databunch(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = report_slicer(TextLMDataBunch,\"../data/\",\n",
    "                                  train_df=train_set,\n",
    "                                  valid_df=val_set,\n",
    "                                  test_df=test_set,\n",
    "                                  text_cols='report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and Loading Data\n",
    "- running `{data_variable}.save({path_to_saved_file})` will save weights of the model, but not the model structure\n",
    "- running `{data_variable} = load_data({path_to_saved_file})` will again load the weights of model if structure is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.save('../data/reports_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = load_data(\"../data/\", 'reports_all.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Encoder Model Parameters Set Here\n",
    "- already set, **vocab size** is set to the length of the dataset vocabulary (after processing) \n",
    "- **encoding_size** = encoding size, yeah\n",
    "- **layer_size** = size of hidden layers\n",
    "- **num layers** = # of hidden layers\n",
    "- rest of parameters (which don't need to be modified) [described here](#us_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(text_data.vocab.itos)\n",
    "encoding_size = 400\n",
    "layer_size = 1152\n",
    "num_layers = 3\n",
    "\n",
    "# stable, these variables don't need to be fine-tuned (described down below)\n",
    "pad_token = 1\n",
    "hidden_dropout = 0.2\n",
    "input_dropout = 0.6\n",
    "embed_dropout = 0.1\n",
    "weight_dropout = 0.5\n",
    "output_dropout = 0.1\n",
    "qrnn_cells = False\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best unsupervised test loss:\n",
    "- Unidirectional = 1.64\n",
    "- Bidirectional = 1.14 (loss was consistently decreasing, not converged but didn't have enough time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Modules - Setup for Unsupervised Models for Training\n",
    "\n",
    "- **Dropout Mask** - Form of inverted-dropout, to make testing easier, based on concept [here](https://deepnotes.io/dropout)\n",
    "- **RNNDropout** - layer with Dropout Mask\n",
    "- **Weight/Embedding Dropout** - Dropout classes for respective layers (can be 'turned off' but best results if enabled), otherwise it overfits like crazy\n",
    "- **\"Split\" functions** allow for gradual fine-tuning of language model and classifier by making the learning rate tiny in the early layers and bigger in the later ones. This idea is from [this paper](https://arxiv.org/pdf/1711.10177.pdf) and is best used for the classifier training, especially on reports with super fine-grained classes, actually does not even take as long\n",
    "- the **config modules** are taken from the [AWD-LSTM paper](https://arxiv.org/pdf/1708.02182.pdf), they're needed if wanna utilize the pretrained models, from WikiText data\n",
    "- [link to split function](https://github.com/fastai/fastai/blob/master/fastai/text/models/awd_lstm.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x:Tensor, sz:Collection[int], p:float):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)\n",
    "\n",
    "class RNNDropout(Module):\n",
    "    def __init__(self, p:float=0.5): self.p=p\n",
    "\n",
    "    def forward(self, x:Tensor)->Tensor:\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        return x * m\n",
    "\n",
    "class WeightDropout(Module):\n",
    "    def __init__(self, module:nn.Module, weight_p:float, layer_names:Collection[str]=['weight_hh_l0']):\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args:ArgStar):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)\n",
    "\n",
    "    def reset(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=False)\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()\n",
    "\n",
    "class EmbeddingDropout(Module):\n",
    "    #Dropout\n",
    "\n",
    "    def __init__(self, emb:nn.Module, embed_p:float):\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words:LongTensor, scale:Optional[float]=None)->Tensor:\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
    "\n",
    "class SequentialRNN(nn.Sequential):\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "\n",
    "def awd_lstm_lm_split(model:nn.Module) -> List[nn.Module]:\n",
    "    groups = [[rnn, dp] for rnn, dp in zip(model[0].rnns, model[0].hidden_dps)]\n",
    "    return groups + [[model[0].encoder, model[0].encoder_dp, model[1]]]\n",
    "\n",
    "def awd_lstm_clas_split(model:nn.Module) -> List[nn.Module]:\n",
    "    groups = [[model[0].module.encoder, model[0].module.encoder_dp]]\n",
    "    groups += [[rnn, dp] for rnn, dp in zip(model[0].module.rnns, model[0].module.hidden_dps)]\n",
    "    return groups + [[model[1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Unsupervised Models: Encoder + Decoder\n",
    "<a id=\"us_en\"></a>\n",
    "\n",
    "#### Encoder\n",
    "#### [Setup Here](#build_encoder)\n",
    "- The encoder can be modified as needed, but the `us_ccds_encoder` class can take in various modifiers pretty nicely\n",
    "- The hyperparameters that are currently the default were the ones that worked well, specifically on the CT Hemorrhage dataset\n",
    "- **vocab_sz** = should not be modified, is automatically set to the vocab size of the processed dataset\n",
    "- **emb_sz** = size of embedding later\n",
    "- **n_hid** = dimensionality of hidden layers\n",
    "- **n_layers** = number of layers\n",
    "- **hidden_p** = dropout % in hidden layers\n",
    "- **input_p** = dropout % in input layers\n",
    "- **embed_p** = drop % in embedding layers\n",
    "- **weight_p** = drop % for weights\n",
    "- **qrnn** = quasi-recurrent cells [based on this paper](https://arxiv.org/pdf/1611.01576.pdf) - supposedly it's a better technique, but it's terrible when I use it, so either something wrong with the way I did it or its just bad\n",
    "- **bidir** - Bidirectional model - got it working now, so it's a thing to try, actually seems to be doing better than the unidirectional one, seems to take longer to train (+10m/epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class us_ccds_encoder(Module):\n",
    "    #Encoder\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz:int=vocab_size, enc_sz:int=encoding_size, n_hid:int=layer_size, n_layers:int=num_layers, pad_token:int=pad_token, hidden_p:float=hidden_dropout,\n",
    "                 input_p:float=input_dropout, embed_p:float=embed_dropout, weight_p:float=weight_dropout, qrnn:bool=qrnn_cells, bidir:bool=bidirectional):\n",
    "        self.bs,self.qrnn,self.enc_sz,self.n_hid,self.n_layers = 1,qrnn,enc_sz,n_hid,n_layers\n",
    "        self.n_dir = 2 if bidir else 1\n",
    "        self.encoder = nn.Embedding(vocab_sz, enc_sz, padding_idx=pad_token)\n",
    "        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
    "        if self.qrnn:\n",
    "            from .qrnn import QRNN\n",
    "            self.rnns = [QRNN(enc_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else enc_sz)//self.n_dir, 1,\n",
    "                              save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True, bidirectional=bidir) \n",
    "                         for l in range(n_layers)]\n",
    "            for rnn in self.rnns: \n",
    "                rnn.layers[0].linear = WeightDropout(rnn.layers[0].linear, weight_p, layer_names=['weight'])\n",
    "        else:\n",
    "            self.rnns = [nn.LSTM(enc_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else enc_sz)//self.n_dir, 1,\n",
    "                                 batch_first=True, bidirectional=bidir) for l in range(n_layers)]\n",
    "            self.rnns = [WeightDropout(rnn, weight_p) for rnn in self.rnns]\n",
    "        self.rnns = nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input:Tensor, from_embeddings:bool=False)->Tuple[Tensor,Tensor]:\n",
    "        if from_embeddings: bs,sl,es = input.size()\n",
    "        else: bs,sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(input if from_embeddings else self.encoder_dp(input))\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden, cpu=False)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def _one_hidden(self, l:int)->Tensor:\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.enc_sz) // self.n_dir\n",
    "        return one_param(self).new(self.n_dir, self.bs, nh).zero_()\n",
    "\n",
    "    def select_hidden(self, idxs):\n",
    "        if self.qrnn: self.hidden = [h[:,idxs,:] for h in self.hidden]\n",
    "        else: self.hidden = [(h[0][:,idxs,:],h[1][:,idxs,:]) for h in self.hidden]\n",
    "        self.bs = len(idxs)\n",
    "\n",
    "    def reset(self):\n",
    "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
    "        if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"us_de\"></a>\n",
    "#### Decoder\n",
    "#### [Setup Here](#build_decoder)\n",
    "- Decoder's simple, just a fully connected layer, out to the vocabulary layer\n",
    "- **n_out** - output size of decoder, should remain the same as the embedding size for encoder, which is the same as the vocab size of the processed dataset\n",
    "- **n_hid** - size of encoding layer from encoder, automatically set\n",
    "- **output_p** = % dropout for last layer\n",
    "- **tie_encoder** - **important** input to connect encoder weights to decoder weights, otherwise gradients aren't computed\n",
    "- **bias** - bias for last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class us_ccds_decoder(Module):\n",
    "    #Decoder\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int=vocab_size, n_hid:int=encoding_size, output_p:float=output_dropout, tie_encoder:nn.Module=None, bias:bool=True):\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1])\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `load_pretrained` - method to load in pretrained models\n",
    "- from online article, loads in pretrained files and transfers weights, still not fully functional | will try and get working by Friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(self, wgts_fname:str, itos_fname:str, strict:bool=True):\n",
    "        old_itos = pickle.load(open(itos_fname, 'rb'))\n",
    "        old_stoi = {v:k for k,v in enumerate(old_itos)}\n",
    "        wgts = torch.load(wgts_fname, map_location=lambda storage, loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model']\n",
    "        wgts = convert_weights(wgts, old_stoi, self.data.train_ds.vocab.itos)\n",
    "        self.model.load_state_dict(wgts, strict=strict)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build_encoder\"></a>\n",
    "#### Build Encoder [(Descriptions of arguments here)](#us_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = us_ccds_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build_decoder\"></a>\n",
    "#### Build Decoder ([Descriptions of arguments here](#us_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = us_ccds_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine encoder + decoder into Unsupervised-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "custom_model = SequentialRNN(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_awd = LanguageLearner(text_data, custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to run on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_awd.model = custom_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_awd.to_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Loss Function if Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn_awd.loss_func=MSELossFlat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find & Plot Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_awd.lr_find()\n",
    "learn_awd.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Training Process\n",
    "- **cyc_len** = epochs/iterations\n",
    "- **max_lr** = learning rate\n",
    "- **callbacks** = saved model with lowest validation loss during training session as 'bestmodel'\n",
    "- **to load best model parameters:** `{model_name}.load('bestmodel')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_awd.fit_one_cycle(cyc_len=1,max_lr=1e-3,callbacks=[SaveModelCallback(learn_awd, every='improvement', monitor='valid_loss')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load bestmodel parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_awd.save('language_modelv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_awd.load('language_modelv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run predict on series of text to explore model language understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn_awd.predict(\"No evidence of a particular\",100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful method to save only `encoder` section of language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_encoder(self, name):\n",
    "        if is_pathlike(name): \n",
    "            self._test_writeable_path()\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if hasattr(encoder, 'module'): \n",
    "            encoder = encoder.module\n",
    "        torch.save(encoder.state_dict(), self.path/self.model_dir/f'{name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_awd.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Supervised NLP - Decoder Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and clean data [using same methods](#proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = '../data/abdoAA_500.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = pd.read_csv(data_file_path);all_labeled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_all_labeled_data = clean_impressions(all_labeled_data,'IMPRESSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train_set, labeled_val_set, labeled_test_set = shuffle_split_dataset(cleaned_all_labeled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_labeled_set\"></a>\n",
    "#### `create_labeled_set` processes the raw text data into model-readable form\n",
    "[`TextLMDataBunch`](https://docs.fast.ai/text.data.html) is a fastai class that manages data for development, and makes it a lot easier to train. It inputs are:\n",
    "\n",
    "- **cls** = class (always TextClasDataBunch)\n",
    "- **path** = directory where data is stored and where saved model files will be saved and loaded\n",
    "- **train/valid/test_df** = development sets from [`shuffle_split_dataset` method](#shuffle_split_dataset) \n",
    "- **text_cols** = the header of the column with text (can be a list of multiple columns of text if necessary, in which case, `mark_fields` should be set to <font color=blue>True</font>\n",
    "- **label_cols** = the header of the column with labels (can be a list of multiple columns of labels if necessary, results in multilabel or multiclass classification depending on dataset.)\n",
    "- **tokenizer** = tokenizer function (set in [`_get_processor`](#token))\n",
    "- **bs** = batch_size of function, usually 128 is the limit (for 32 GB of GPUs)\n",
    "- **vocab** = will automatically create initially, required to be set to the same vocab as the previous dataframe, generated by the `report_slicer` method \n",
    "- if there is an error while training, relating to the dataframe, make sure that the processing variables not above are set to the same values as the encoder dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@add_method(TextClasDataBunch)\n",
    "def create_labeled_set(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "            tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "            label_cols:IntsOrStrs=0, label_delim:str=None, chunksize:int=10000, max_vocab:int=60000,\n",
    "            min_freq:int=2, mark_fields:bool=False, include_bos:bool=True, include_eos:bool=False, **kwargs) -> DataBunch:\n",
    "    processor = _get_processor(tokenizer=tokenizer, vocab=vocab, chunksize=chunksize, max_vocab=max_vocab,\n",
    "                               min_freq=min_freq, mark_fields=mark_fields, \n",
    "                               include_bos=include_bos, include_eos=include_eos)\n",
    "    if classes is None and is_listy(label_cols) and len(label_cols) > 1: \n",
    "        classes = label_cols\n",
    "    src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                    TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "    if cls==TextLMDataBunch: \n",
    "        src = src.label_for_lm()\n",
    "    else: \n",
    "        if label_delim is not None: \n",
    "            src = src.label_from_df(cols=label_cols, classes=classes, label_delim=label_delim)\n",
    "        else: src = src.label_from_df(cols=label_cols, classes=classes)\n",
    "    if test_df is not None: \n",
    "        src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "    return src.databunch(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeled = create_labeled_set(TextClasDataBunch,\n",
    "                             path='../data/',\n",
    "                             train_df=labeled_train_set,\n",
    "                             valid_df=labeled_val_set,\n",
    "                             test_df=labeled_test_set,\n",
    "                             text_cols='IMPRESSION',\n",
    "                             label_cols='cohort',\n",
    "                             vocab=text_data.train_ds.vocab,\n",
    "                             bs=128\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving and Loading Data\n",
    "- running `{data_variable}.save({path_to_saved_file})` will save weights of the model, but not the model structure\n",
    "- running `{data_variable} = load_data({path_to_saved_file})` will again load the weights of model if structure is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeled.save('../data/AAA.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeled = load_data('../data','AAA.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Decoder Model Parameters Set Here\n",
    "- **encoding_size** = encoding size from encoder training\n",
    "- **num_classes** = number of classes, yeah\n",
    "- **decoder_layer_sizes** = the sizes of the layers of decoder, can be variable length, just pass any length list of layer sizes\n",
    "- **decoder_dropout** = the corresponding dropout values for the `decoder_layer_sizes`\n",
    "- **bptt_classifier** = backprop through time, number of samples before gradient calculation\n",
    "- **max_length** = maximum length of text to input into algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "decoder_layer_sizes =[50]\n",
    "decoder_dropout = [0.1]\n",
    "bptt_classifier = 70\n",
    "max_length = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Classifier Results (Test Set)\n",
    "#### Binary:\n",
    "- **CT Hemorrhage** = AUC:**0.99**\n",
    "- **Occlusion** = AUC: **0.98**\n",
    "- **Stroke** = AUC: **0.95**\n",
    "\n",
    "#### Multiclass:\n",
    "- CT Hemorrhage:\n",
    "    - **EDH** = RMSE: **1.52**\n",
    "    - **IPH** = RMSE: **0.84**\n",
    "    - **IVH** = RMSE: **1.37**\n",
    "    - **SAH** = RMSE: **1.65**\n",
    "    - **SDH** = RMSE: **2.02**\n",
    "    \n",
    "- Occlusion:\n",
    "    - **M1** = AUC: **0.95**\n",
    "    - **M2** = AUC: **0.93**\n",
    "    - **ICA** = AUC: **0.92**\n",
    "    - **Basilar** = AUC: **0.84**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `masked_concat_pool`:\n",
    "- based on [this paper](https://arxiv.org/pdf/1406.4729.pdf)\n",
    "- strategy developed for usage on ConvNets, but increased the performance of this model quite a lot (+0.1 AUC)\n",
    "- concatenates max and average pools - **adaptive max pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_concat_pool(outputs, mask):\n",
    "    output = outputs[-1]\n",
    "    avg_pool = output.masked_fill(mask[:, :, None], 0).mean(dim=1)\n",
    "    avg_pool *= output.size(1) / (output.size(1)-mask.type(avg_pool.dtype).sum(dim=1))[:,None]\n",
    "    max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "    x = torch.cat([output[:,-1], max_pool, avg_pool], 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for Supervised Models\n",
    "<a id=\"s_de\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "- `MultiBatchEncoder` transforms the standard report encoder model trained in the previous section by wrapping it to process variable length strings and export a singular encoding, which is then tossed into the decoder model\n",
    "- **bptt** = backprop over time, samples before gradients are calculated\n",
    "- **max_len** = maximum length of text data input, best if kept large, but biggest constraints are memory related + maybe want to restrict sentence size\n",
    "- `module` = preset, takes in initial model to convert to `MultiBatchEncoder`, set to encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBatchEncoder(Module):\n",
    "    def __init__(self, bptt:int, max_len:int, module:nn.Module, pad_idx:int=1):\n",
    "        self.max_len,self.bptt,self.module,self.pad_idx = max_len,bptt,module,pad_idx\n",
    "\n",
    "    def concat(self, arrs:Collection[Tensor])->Tensor:\n",
    "        return [torch.cat([l[si] for l in arrs], dim=1) for si in range_of(arrs[0])]\n",
    "\n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()\n",
    "\n",
    "    def forward(self, input:LongTensor)->Tuple[Tensor,Tensor]:\n",
    "        bs,sl = input.size()\n",
    "        self.reset()\n",
    "        raw_outputs,outputs,masks = [],[],[]\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            r, o = self.module(input[:,i: min(i+self.bptt, sl)])\n",
    "            if i>(sl-self.max_len):\n",
    "                masks.append(input[:,i: min(i+self.bptt, sl)] == self.pad_idx)\n",
    "                raw_outputs.append(r)\n",
    "                outputs.append(o)\n",
    "        return self.concat(raw_outputs),self.concat(outputs),torch.cat(masks,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `us_ccds_encoder` builds decoder\n",
    "- **bptt** = backprop through time, number of samples before gradients calculated\n",
    "- **max_len** = max length of sequence of text\n",
    "- *pretrained* = *still in progress,* will allow for loading of pretrained external model on PubMed\n",
    "- **custom_model** = name of custom model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def su_ccds_encoder(bptt:int=bptt_classifier, \n",
    "                    max_len:int=max_length,\n",
    "                    custom_model=None,\n",
    "                    pad_idx=pad_token) -> nn.Module:\n",
    "    \n",
    "    encoder = MultiBatchEncoder(bptt, max_len, custom_model.cuda(), pad_idx=pad_idx)\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = learn_awd.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = su_ccds_encoder(custom_model=pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `us_ccds_decoder` builds decoder\n",
    "- **vocab_sz** = vocab size for dataset, leave unchanged\n",
    "- **n_class** = number of classes, automaticaaly set from variable\n",
    "- **lin_ftrs** = list of size of layers in decoder, can be variable length to create variable depth decoders\n",
    "- **output_p** = list of dropout % that is equal to `len(lin_ftrs)` otherwise set all to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def su_ccds_decoder(n_class:int=num_classes,\n",
    "                    lin_ftrs:Collection[int]=decoder_layer_sizes,\n",
    "                    encoding_sz:int=encoding_size,\n",
    "                    ps:Collection[float]=decoder_dropout) -> nn.Module:\n",
    "    if ps is None:  \n",
    "        ps = [0.1]*len(lin_ftrs)\n",
    "    layers = [encoding_size * 3] + lin_ftrs + [n_class]\n",
    "    ps = [0.1] + ps\n",
    "    decoder = PoolingLinearClassifier(layers, ps)\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = su_ccds_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `text_classifier_learner` is the function that creates the combined supervised model\n",
    "- **data** = data_class object that contains processed data\n",
    "- **encoder** = encoder model\n",
    "- **decoder** = decoder model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classifier(data:DataBunch,\n",
    "                            encoder=encoder,\n",
    "                            decoder=decoder,\n",
    "                            **learn_kwargs) -> 'TextClassifierLearner':\n",
    "    model = SequentialRNN(encoder, decoder)\n",
    "    learn = RNNLearner(data, model, split_func=awd_lstm_clas_split, **learn_kwargs)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier(data_labeled, metrics = [AUROC()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze the encoder model (`learn.unfreeze()` allows for fine-tuning of encoder model, too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_awd.lr_find()\n",
    "learn_awd.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Training Process\n",
    "- **cyc_len** = epochs/iterations\n",
    "- **max_lr** = learning rate\n",
    "- **callbacks** = saved model with lowest validation loss during training session as 'bestmodel'\n",
    "- **to load best model parameters:** `{model_name}.load('bestmodel')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(cyc_len=50,max_lr=1e-3,callbacks=[SaveModelCallback(learn_awd, every='improvement', monitor='valid_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('AAA_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('AAA_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on string easily with `{model_name}.predict({string})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict('Pizza time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict('Stable postsurgical appearance of graft replacement of the ascending aorta, arch, and descending thoracic aorta. Minimal decrease in size of fluid collection surrounding the descending thoracic graft and adjacent small pleural effusion.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.predict('No evidence of abdominal aortic aneurysm.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good command if GPU util blows up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
