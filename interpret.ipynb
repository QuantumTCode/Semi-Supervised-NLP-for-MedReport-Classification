{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages (fastai for optimizing inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture.quick_load import *\n",
    "from architecture.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = load_data(\"../data/\", 'reports_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = load_data('../data','AAA.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in saved final model\n",
    "- `load_ccds_model` is a function in the architecture folder that automatically loads in the ccds model, but must be customized depending on the parameter changes made during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn = load_ccds_model(text_data=text_data,\n",
    "                labeled_data=labeled_data,\n",
    "                path_to_lm='language_modelv1',\n",
    "                path_to_classifier='AAA',\n",
    "                decoder_layer_sizes=[50],\n",
    "                decoder_dropout=[0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation model: from PyTorch --> FastAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretLearn = TextClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix Operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(self, slice_size=1):\n",
    "    x=torch.arange(0,self.data.c)\n",
    "    if slice_size is None: cm = ((self.pred_class==x[:,None]) & (self.y_true==x[:,None,None])).sum(2)\n",
    "    else:\n",
    "        cm = torch.zeros(self.data.c, self.data.c, dtype=x.dtype)\n",
    "        for i in range(0, self.y_true.shape[0], slice_size):\n",
    "            cm_slice = ((self.pred_class[i:i+slice_size]==x[:,None]) & (self.y_true[i:i+slice_size]==x[:,None,None])).sum(2)\n",
    "            torch.add(cm, cm_slice, out=cm)\n",
    "    return to_np(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(interpretLearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_dropouts(mod):\n",
    "        module_name =  mod.__class__.__name__\n",
    "        if 'Dropout' in module_name or 'BatchNorm' in module_name: mod.training = False\n",
    "        for module in mod.children(): _eval_dropouts(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrinsic_perturb(self, text, class_id=None):\n",
    "    self.model.train()\n",
    "    _eval_dropouts(self.model)\n",
    "    self.model.zero_grad()\n",
    "    self.model.reset()\n",
    "    ids = self.data.one_item(text)[0]\n",
    "    emb = self.model[0].module.encoder(ids).detach().requires_grad_(True)\n",
    "    lstm_output = self.model[0].module(emb, from_embeddings=True)\n",
    "    self.model.eval()\n",
    "    cl = self.model[1](lstm_output + (torch.zeros_like(ids).byte(),))[0].softmax(dim=-1)\n",
    "    if class_id is None: \n",
    "        class_id = cl.argmax()\n",
    "    cl[0][class_id].backward()\n",
    "    attn = emb.grad.squeeze().abs().sum(dim=-1)\n",
    "    attn /= attn.max()\n",
    "    tokens = self.data.single_ds.reconstruct(ids[0])\n",
    "    return tokens, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intrinsic_perturb(interpretLearn, 'Stable postsurgical appearance of graft replacement of the ascending aorta, arch, and descending thoracic aorta. Minimal decrease in size of fluid collection surrounding the descending thoracic graft and adjacent small pleural effusion.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Document Encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_generator(self, text:str, n_words:int=1, no_unk:bool=True, temperature:float=1., min_p:float=None, sep:str=' ',\n",
    "            decoder=decode_spec_tokens):\n",
    "    ds = self.data.single_dl.dataset\n",
    "    xb,yb = self.data.one_item(text)\n",
    "    new_idx = []\n",
    "    x = 0\n",
    "    x = pred_batch(self,batch=(xb,yb))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss_func_name2activ(name:str, axis:int=-1):\n",
    "    res = loss_func_name2activ[name]\n",
    "    if res == F.softmax: res = partial(F.softmax, dim=axis)\n",
    "    return res\n",
    "\n",
    "loss_func_name2activ = {'cross_entropy_loss': F.softmax, 'nll_loss': torch.exp, 'poisson_nll_loss': torch.exp,\n",
    "    'kl_div_loss': torch.exp, 'bce_with_logits_loss': torch.sigmoid, 'cross_entropy': F.softmax,\n",
    "    'kl_div': torch.exp, 'binary_cross_entropy_with_logits': torch.sigmoid,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss_func2activ(loss_func):\n",
    "    if getattr(loss_func,'keywords',None):\n",
    "        if not loss_func.keywords.get('log_input', True): return\n",
    "    axis = getattr(loss_func, 'axis', -1)\n",
    "    # flattened loss\n",
    "    loss_func = getattr(loss_func, 'func', loss_func)\n",
    "    # could have a partial inside flattened loss! Duplicate on purpose.\n",
    "    loss_func = getattr(loss_func, 'func', loss_func)\n",
    "    cls_name = camel2snake(loss_func.__class__.__name__)\n",
    "    if cls_name == 'mix_up_loss':\n",
    "        loss_func = loss_func.crit\n",
    "        cls_name = camel2snake(loss_func.__class__.__name__)\n",
    "    if cls_name in loss_func_name2activ:\n",
    "        if cls_name == 'poisson_nll_loss' and (not getattr(loss_func, 'log_input', True)): return\n",
    "        return _loss_func_name2activ(cls_name, axis)\n",
    "    if getattr(loss_func,'__name__','') in loss_func_name2activ:\n",
    "        return _loss_func_name2activ(loss_func.__name__, axis)\n",
    "    return noop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model:nn.Module, xb:Tensor, yb:Tensor, loss_func:OptLossFunc=None, opt:OptOptimizer=None,\n",
    "               cb_handler:Optional[CallbackHandler]=None)->Tuple[Union[Tensor,int,float,str]]:\n",
    "    \"Calculate loss and metrics for a batch, call out to callbacks as necessary.\"\n",
    "    cb_handler = ifnone(cb_handler, CallbackHandler())\n",
    "    if not is_listy(xb): xb = [xb]\n",
    "    if not is_listy(yb): yb = [yb]\n",
    "    \n",
    "    out = model(*xb)\n",
    "    return out[1][-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_batch(self, ds_type:DatasetType=DatasetType.Valid, batch:Tuple=None, reconstruct:bool=False, with_dropout:bool=False) -> List[Tensor]:\n",
    "        if batch is not None: xb,yb = batch\n",
    "        else: xb,yb = self.data.one_batch(ds_type, detach=False, denorm=False)\n",
    "        cb_handler = CallbackHandler(self.callbacks)\n",
    "        xb,yb = cb_handler.on_batch_begin(xb,yb, train=False)\n",
    "        with torch.no_grad():\n",
    "            if not with_dropout: preds = loss_batch(self.model.eval(), xb, yb, cb_handler=cb_handler)\n",
    "            else: preds = loss_batch(self.model.eval().apply(self.apply_dropout), xb, yb, cb_handler=cb_handler)\n",
    "            res = _loss_func2activ(self.loss_func)(preds[0])\n",
    "        if not reconstruct: return res\n",
    "        res = res.detach().cpu()\n",
    "        ds = self.dl(ds_type).dataset\n",
    "        norm = getattr(self.data, 'norm', False)\n",
    "        if norm and norm.keywords.get('do_y',False):\n",
    "            res = self.data.denorm(res, do_x=True)\n",
    "        return [ds.reconstruct(o) for o in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Generate Document Encodings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = encoding_generator(learn,'diseases present is this file hemorrhage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to get test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score,y_test=learn.get_preds(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.numpy()\n",
    "y_score = y_score.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score[:,1], pos_label=1,)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.30f)' % auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Classification Impressions')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import fbeta_score\n",
    "def opt_th(preds, targs, start=0.01, end=1, step=0.01):\n",
    "    ths = np.arange(start,end,step)\n",
    "    thresholds=[]\n",
    "    thres = 0\n",
    "    best_score = 0\n",
    "    for th in ths:\n",
    "        thresholds.append(fbeta_score(targs, (preds>th), 2, average='binary'))\n",
    "        if fbeta_score(targs, (preds>th), 2, average='binary') > best_score:\n",
    "            best_score = fbeta_score(targs, (preds>th), 2, average='binary')\n",
    "    idx = np.argmax(thresholds)\n",
    "    print('Best threshold = ', ths[idx])\n",
    "    print('Best F-Score = ', best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_th(y_score[:,1], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
